# ============================================================================
# Amazon Smart Shopping Assistant - Configuration
# ============================================================================
# API keys are loaded from .env file (not stored here)

# LangSmith Configuration (optional monitoring/tracing)
langsmith:
  enabled: true
  project_name: "E-Commerce Agentic AI"
  endpoint: "https://api.smith.langchain.com/"
  tracing_v2: true

# ============================================================================
# Agent-Specific Configurations
# ============================================================================
# 8 Agents in execution order:
#   1. Router → 2. Planning → 3. Collection → 
#   4. Review (parallel) | 5. Market (parallel) | 6. Price (parallel) →
#   7. Analysis → 8. Response
#
# Collection Agent does NOT use LLM (API-only for SerpAPI calls)
# ============================================================================

agents:
  # ─────────────────────────────────────────────────────────────────────────
  # 0. Manager Agent (Orchestrator) - Mission Control
  # ─────────────────────────────────────────────────────────────────────────
  # Prompts: ai_server/prompts/manager_agent_prompts.md
  manager:
    provider: "cerebras"
    model_name: "qwen-3-32b"
    temperature: 0.1
    max_tokens: 4000

  # ─────────────────────────────────────────────────────────────────────────
  # 1. Search Agent (The Hunter)
  # ─────────────────────────────────────────────────────────────────────────
  # Wraps Collection Agent logic
  search:
    provider: "cerebras" # Not used for search yet, but good for future
    model_name: "qwen-3-32b"
    temperature: 0.1

  # ─────────────────────────────────────────────────────────────────────────
  # 2. Advisor Agent (The Expert)
  # ─────────────────────────────────────────────────────────────────────────
  advisor:
    provider: "cerebras"
    model_name: "qwen-3-32b"
    temperature: 0.1
    max_tokens: 4000

  # ─────────────────────────────────────────────────────────────────────────
  # 3. Reviewer Agent (The Critic)
  # ─────────────────────────────────────────────────────────────────────────
  reviewer:
    provider: "cerebras"
    model_name: "zai-glm-4.6"
    temperature: 0.1
    max_tokens: 4000

# ============================================================================
# LLM Fallback Configuration
# ============================================================================
# Order: Cerebras → Gemini → OpenAI
llm_fallback:
  cerebras:
    model_name: "qwen-3-32b"
    temperature: 0.1
    max_tokens: 2000
  gemini:
    model_name: "models/gemini-2.0-flash-lite"
    temperature: 0.1
    max_tokens: 8000
  openai:
    model_name: "gpt-4o-mini"
    temperature: 0.1
    max_tokens: 8000


# ============================================================================
# SerpAPI Configuration
# ============================================================================
serpapi:
  engine: "amazon"
  timeout: 30
  max_results: 20

# ============================================================================
# Memory & Personalization Configuration (Phase 3)
# ============================================================================
memory:
  # Storage backend
  storage:
    backend: "sqlite"  # sqlite, redis (future)
    sqlite:
      db_path: "data/sessions.db"
  
  # Conversation history
  conversation:
    max_turns: 10  # Keep last N turns in active memory
    summarize_after: 5  # Summarize if conversation exceeds N turns
  
  # Preference learning
  preferences:
    min_confidence: 0.3  # Minimum confidence to apply preference
    learning_rate: 0.1  # How quickly preferences are learned
    decay_factor: 0.9  # Older preferences decay over time
  
  # Session management
  session:
    default_ttl: 3600  # Default session TTL: 1 hour
    max_ttl: 86400  # Maximum session TTL: 24 hours
    cleanup_interval: 300  # Clean expired sessions every 5 minutes

# ============================================================================
# Vector Store Configuration
# ============================================================================
vector_store:
  # Backend: faiss (recommended), chroma (legacy)
  backend: "faiss"
  
  # FAISS configuration
  faiss:
    index_path: "data/faiss_index"
    # Index type: flat (exact), ivf (approximate), hnsw (graph-based)
    index_type: "flat"
    # Metric: cosine, l2 (euclidean), ip (inner product)
    metric: "cosine"
    # Number of clusters for IVF index (only used if index_type=ivf)
    nlist: 100
    # Number of probes for IVF search (only used if index_type=ivf)
    nprobe: 10

# ============================================================================
# Knowledge Graph Configuration
# ============================================================================
knowledge_graph:
  enabled: true
  
  # Storage backend
  storage:
    backend: "sqlite"  # sqlite (default), neo4j (future)
    sqlite:
      db_path: "data/knowledge_graph.db"
    # neo4j:
    #   uri: "bolt://localhost:7687"
    #   user: "neo4j"
    #   password: ""
  
  # Entity extraction settings
  extraction:
    provider: "cerebras"
    model_name: "qwen-3-32b"
    temperature: 0.1
    min_confidence: 0.7
    max_entities_per_query: 15
    batch_size: 5
  
  # Retrieval settings
  retrieval:
    max_hops: 2  # Graph traversal depth
    top_k_entities: 10
    include_relationships: true
    use_embeddings: true  # Hybrid: embedding + graph

# ============================================================================
# Knowledge Base Configuration (Policies/FAQs)
# ============================================================================
knowledge_base:
  enabled: true
  collection_name: "knowledge_base"
  data_path: "data/policy_faq.json"
  
  # Search settings
  search:
    default_k: 5
    use_language_filter: true
    fallback_without_language: true
  
  # Auto-initialization
  auto_load_on_startup: true
  force_reload: false  # Set to true to reload data on every startup

# ============================================================================
# Embedding Model Configuration
# ============================================================================
embeddings:
  # Provider: huggingface, sentence_transformers, openai
  provider: "huggingface"
  
  # Model configuration
  model_name: "Qwen/Qwen3-Embedding-0.6B"
  # Embedding dimension (Qwen3-Embedding-0.6B = 1024)
  dimension: 1024
  # Batch size for encoding
  batch_size: 32
  # Device: auto, cpu, cuda, mps
  device: "auto"
  # Trust remote code (required for some models)
  trust_remote_code: true
  # Normalize embeddings (recommended for cosine similarity)
  normalize: true
  # Max sequence length
  max_length: 512