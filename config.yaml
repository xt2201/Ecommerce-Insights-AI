# ============================================================================
# Amazon Smart Shopping Assistant - Configuration
# ============================================================================
# API keys are loaded from .env file (not stored here)

# LangSmith Configuration (optional monitoring/tracing)
langsmith:
  enabled: true
  project_name: "E-Commerce Agentic AI"
  endpoint: "https://api.smith.langchain.com/"
  tracing_v2: true

# ============================================================================
# Agent-Specific Configurations
# ============================================================================
# 8 Agents in execution order:
#   1. Router → 2. Planning → 3. Collection → 
#   4. Review (parallel) | 5. Market (parallel) | 6. Price (parallel) →
#   7. Analysis → 8. Response
#
# Collection Agent does NOT use LLM (API-only for SerpAPI calls)
# ============================================================================

agents:
  # ─────────────────────────────────────────────────────────────────────────
  # 1. Router Agent - Query Classification & Routing
  # ─────────────────────────────────────────────────────────────────────────
  # Routes to: direct_search | planning | clarification
  # Prompts: ai_server/prompts/router_agent_prompts.md
  router:
    provider: "cerebras"
    model_name: "qwen-3-32b"
    temperature: 0.0
    max_tokens: 2000
  
  # ─────────────────────────────────────────────────────────────────────────
  # 2. Planning Agent - Search Strategy Builder
  # ─────────────────────────────────────────────────────────────────────────
  # Uses tools in: ai_server/tools/planning_tools.py
  # Prompts: ai_server/prompts/planning_agent_prompts.md
  planning:
    provider: "cerebras"
    model_name: "qwen-3-32b"
    temperature: 0.1
    max_tokens: 4000
    max_retries: 2
  
  # ─────────────────────────────────────────────────────────────────────────
  # 3. Collection Agent - SerpAPI Product Fetching
  # ─────────────────────────────────────────────────────────────────────────
  # NOTE: Does NOT use LLM - pure API calls to SerpAPI

  
  # ─────────────────────────────────────────────────────────────────────────
  # 4. Review Intelligence Agent (PARALLEL)
  # ─────────────────────────────────────────────────────────────────────────
  # Prompts: ai_server/prompts/review_agent_prompts.md
  review:
    provider: "cerebras"
    model_name: "zai-glm-4.6"
    temperature: 0.1
    max_tokens: 4000

  # ─────────────────────────────────────────────────────────────────────────
  # 5. Market Intelligence Agent (PARALLEL)
  # ─────────────────────────────────────────────────────────────────────────
  # Prompts: ai_server/prompts/market_agent_prompts.md
  market:
    provider: "cerebras"
    model_name: "qwen-3-32b"
    temperature: 0.1
    max_tokens: 4000

  # ─────────────────────────────────────────────────────────────────────────
  # 6. Price Tracking Agent (PARALLEL)
  # ─────────────────────────────────────────────────────────────────────────
  # Prompts: ai_server/prompts/price_agent_prompts.md
  price:
    provider: "cerebras"
    model_name: "llama-3.3-70b"
    temperature: 0.1
    max_tokens: 2000

  # ─────────────────────────────────────────────────────────────────────────
  # 7. Analysis Agent - Chain-of-Thought Reasoning
  # ─────────────────────────────────────────────────────────────────────────
  # Prompts: ai_server/prompts/analysis_agent_prompts.md
  analysis:
    provider: "cerebras"
    model_name: "gpt-oss-120b"
    temperature: 0.2
    max_tokens: 8000
    max_products_to_analyze: 5
  
  # ─────────────────────────────────────────────────────────────────────────
  # 8. Response Agent - Final Report Generation
  # ─────────────────────────────────────────────────────────────────────────
  # Prompts: ai_server/prompts/response_agent_prompts.md
  response:
    provider: "cerebras"
    model_name: "zai-glm-4.6"
    temperature: 0.3
    max_tokens: 20000
    max_recommendations: 5

# ============================================================================
# LLM Fallback Configuration
# ============================================================================
# Order: Cerebras → Gemini → OpenAI
llm_fallback:
  cerebras:
    model_name: "qwen-3-32b"
    temperature: 0.1
    max_tokens: 2000
  gemini:
    model_name: "models/gemini-2.0-flash-lite"
    temperature: 0.1
    max_tokens: 8000
  openai:
    model_name: "gpt-4o-mini"
    temperature: 0.1
    max_tokens: 8000


# ============================================================================
# SerpAPI Configuration
# ============================================================================
serpapi:
  engine: "amazon"
  timeout: 30
  max_results: 20

# ============================================================================
# Memory & Personalization Configuration (Phase 3)
# ============================================================================
memory:
  # Storage backend
  storage:
    backend: "sqlite"  # sqlite, redis (future)
    sqlite:
      db_path: "data/sessions.db"
  
  # Conversation history
  conversation:
    max_turns: 10  # Keep last N turns in active memory
    summarize_after: 5  # Summarize if conversation exceeds N turns
  
  # Preference learning
  preferences:
    min_confidence: 0.3  # Minimum confidence to apply preference
    learning_rate: 0.1  # How quickly preferences are learned
    decay_factor: 0.9  # Older preferences decay over time
  
  # Session management
  session:
    default_ttl: 3600  # Default session TTL: 1 hour
    max_ttl: 86400  # Maximum session TTL: 24 hours
    cleanup_interval: 300  # Clean expired sessions every 5 minutes

# ============================================================================
# Vector Store Configuration
# ============================================================================
vector_store:
  # Backend: faiss (recommended), chroma (legacy)
  backend: "faiss"
  
  # FAISS configuration
  faiss:
    index_path: "data/faiss_index"
    # Index type: flat (exact), ivf (approximate), hnsw (graph-based)
    index_type: "flat"
    # Metric: cosine, l2 (euclidean), ip (inner product)
    metric: "cosine"
    # Number of clusters for IVF index (only used if index_type=ivf)
    nlist: 100
    # Number of probes for IVF search (only used if index_type=ivf)
    nprobe: 10

# ============================================================================
# Embedding Model Configuration
# ============================================================================
embeddings:
  # Provider: huggingface, sentence_transformers, openai
  provider: "huggingface"
  
  # Model configuration
  model_name: "Qwen/Qwen3-Embedding-0.6B"
  # Embedding dimension (Qwen3-Embedding-0.6B = 1024)
  dimension: 1024
  # Batch size for encoding
  batch_size: 32
  # Device: auto, cpu, cuda, mps
  device: "auto"
  # Trust remote code (required for some models)
  trust_remote_code: true
  # Normalize embeddings (recommended for cosine similarity)
  normalize: true
  # Max sequence length
  max_length: 512