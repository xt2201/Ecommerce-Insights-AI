# Amazon Smart Shopping Assistant

> ğŸš€ **100% Agentic AI** - LangGraph + LLM-First Architecture + **RAG for Policies/FAQs**

Multi-agent AI system with **100% LLM-powered decision making**, **intelligent 10+ message context retention**, and **bilingual policy/FAQ RAG** (FAISS + Knowledge Graph).

**ğŸ‰ Status:** Production Ready | **Version:** 8.0.0 (Agentic + RAG)

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com/)
[![Next.js](https://img.shields.io/badge/Next.js-14-black.svg)](https://nextjs.org/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

---

## âš¡ Quick Start

```bash
# 1. Clone & setup
git clone git@github.com:xt2201/Ecommerce-Insights-AI.git
cd Ecommerce-Insights-AI

# 2. Configure API keys in .env
cp .env.example .env
# Edit .env with your API keys:
# SERPAPI_API_KEY=your_key
# CEREBRAS_API_KEY=your_key

# 3. Start backend (Local Mode)
./start_w0_docker.sh

# OR with Docker
./start.sh

# 4. Open http://localhost:3000
```

---

## ğŸ›’ V8.0: RAG for Policies/FAQs + Knowledge Graph

**NEW**: Bilingual FAQ/policy system with hybrid RAG (FAISS semantic search + SQLite knowledge graph).

### FAQ Example
```
User: "How long does shipping take?"
Bot:  â†’ Searches KnowledgeBase (34 policies/FAQs)
      â†’ Retrieves: "Shipping Policy" (3-5 business days)
      â†’ Returns: "Standard shipping takes 3-5 business days..."

User: "LÃ m sao Ä‘á»ƒ Ä‘á»•i tráº£?" (Vietnamese)
Bot:  â†’ Auto-detects Vietnamese
      â†’ Searches Vietnamese documents
      â†’ Returns: "Báº¡n cÃ³ thá»ƒ Ä‘á»•i tráº£ trong vÃ²ng 30 ngÃ y..."
```

### V8 RAG Features
| Feature | Implementation |
|---------|----------------|
| **Bilingual Support** | EN/VI with auto language detection |
| **Semantic Search** | FAISS vector search (Qwen3-Embedding-0.6B) |
| **Knowledge Graph** | SQLite with 11 entity types, 13 relationship types |
| **Entity Extraction** | LLM-powered with confidence scoring |
| **Hybrid RAG** | Vector search + graph traversal (BFS, max_hops=2) |
| **Data** | 34 documents (16 policies + 18 FAQs) |

## ğŸ›’ V7.0: Session Persistence + 10+ Message Context

The system maintains conversation context across 10+ messages with persistent session memory.

### Example Multi-Turn Conversation
```
User: "hello"
Bot:  "Hi! I'm Alex, your shopping assistant!"

User: "tÃ´i muá»‘n mua giÃ y"
Bot:  "What type of shoes? Running, casual, formal?"

User: "sneaker"
Bot:  "Great! Any brand preference? Use case?"

User: "tÃ´i dÃ¹ng Ä‘á»ƒ cháº¡y bá»™, kiá»ƒu thá»ƒ thao nhÆ° adidas"
Bot:  â†’ 60 running shoes found

User: "tÃ´i giá»›i tÃ­nh nam"  â† Previously caused context loss!
Bot:  â†’ 60 MEN'S running shoes âœ… Context preserved!

User: "mÃ u Ä‘en hoáº·c xanh"
Bot:  â†’ 48 products with black/blue filter âœ…

User: "giÃ¡ dÆ°á»›i $150"
Bot:  â†’ 59 products under $150 âœ…

User: "ok tÃ¬m Ä‘i"
Bot:  â†’ "Here are Adidas men's running shoes in black/blue under $150" âœ…
```

### V7 Improvements
| Feature | V6 | V7 |
|---------|----|----|
| **Session Persistence** | âŒ Lost between requests | âœ… SQLite storage |
| **Refinement Detection** | âŒ Only LLM-based | âœ… Pattern-based fallback |
| **Context Retention** | ~5 messages | **10+ messages** |
| **Session ID Handling** | âŒ Generated new each time | âœ… Uses provided ID |

---

## ğŸ—ï¸ Architecture: 100% Agentic AI

```mermaid
graph TD
    User[User Message] --> API[FastAPI Server]
    API --> SM[SessionManager + SQLite]
    SM --> Memory[SessionMemory]
    Memory --> Understand[QueryUnderstandingAgent]
    
    Understand --> Router[LLMRouter + Completeness]
    
    Router -->|faq| FAQ[FAQNode + RAG]
    Router -->|incomplete| Clarify[ClarificationNode]
    Router -->|partial| PreConsult[PreSearchConsultationNode]
    Router -->|complete| Search[SearchNode]
    Router -->|consultation| Consult[ConsultationNode]
    
    FAQ -->|Vector Search| KB[KnowledgeBase]
    FAQ -->|Graph Query| KG[KnowledgeGraph]
    
    Search --> Synthesize[SynthesizeNode]
    
    style SM fill:#ffd,stroke:#333
    style Router fill:#bbf,stroke:#333
    style KB fill:#afa,stroke:#333
    style KG fill:#faa,stroke:#333
```

### Core Agentic Components

| Component | Purpose | Details |
|-----------|---------|---------|
| **QueryUnderstandingAgent** | Intent detection | `is_refinement_only` field for refinement detection |
| **LLMRouter** | Route by completeness | <40%â†’clarify, 40-70%â†’consult, >70%â†’search, faqâ†’faq_node |
| **SessionManager** | Persist sessions | SQLite storage, session_id preservation |
| **KnowledgeBase** | Policy/FAQ search | FAISS vector search with bilingual support |
| **KnowledgeGraph** | Entity relationships | SQLite graph with BFS traversal (max_hops=2) |
| **EntityExtractor** | Extract from text | LLM-powered with 11 entity types, 13 relationships |
| **LLM Refinement Detection** | Constraint detection | LLM sets `is_refinement_only=true` for attribute-only messages |
| **TranslationService** | Vietnameseâ†’English | LLM + cache |

---

## ğŸ¯ Features

### AI Server
- ğŸ§  **10+ Message Context**: Persistent session memory with SQLite
- ï¿½ **RAG for Policies/FAQs**: FAISS semantic search + Knowledge Graph (34 documents)
- ğŸŒ **Bilingual Support**: EN/VI with auto language detection
- ğŸ”„ **Pattern-Based Refinement**: Catches LLM misclassifications
- ğŸ¤ **Consultative Shopping**: Helps users who don't know what they want
- ğŸ¤– **Entity Extraction**: LLM-powered with 11 entity types, 13 relationship types
- âš¡ **Cerebras Ultra-Fast**: Sub-second LLM calls
- ğŸ” **Multi-Provider Fallback**: Cerebras â†’ Gemini â†’ OpenAI

### Frontend
- âœ… Modern chat interface
- âœ… Real-time streaming
- âœ… Product cards with pricing

---

## ğŸ“ Project Structure

```
ecom/
â”œâ”€â”€ ai_server/
â”‚   â”œâ”€â”€ agents/                         # 7 AI agents
â”‚   â”‚   â”œâ”€â”€ query_understanding_agent.py
â”‚   â”‚   â”œâ”€â”€ llm_router.py
â”‚   â”‚   â”œâ”€â”€ clarification_agent.py
â”‚   â”‚   â”œâ”€â”€ search_agent.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ rag/                            # RAG components (V8)
â”‚   â”‚   â”œâ”€â”€ knowledge_base.py           # FAISS semantic search
â”‚   â”‚   â”œâ”€â”€ knowledge_graph.py          # SQLite graph storage
â”‚   â”‚   â”œâ”€â”€ entity_extractor.py         # LLM entity extraction
â”‚   â”‚   â””â”€â”€ graph_storage/              # Storage backends
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ session_manager.py          # Session persistence (V7)
â”‚   â”‚   â””â”€â”€ storage/sqlite_storage.py   # SQLite backend
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ knowledge_graph_models.py   # Entity/relationship models (V8)
â”‚   â”œâ”€â”€ graphs/
â”‚   â”‚   â””â”€â”€ shopping_graph.py           # LangGraph workflow (includes faq_node)
â”‚   â”œâ”€â”€ prompts/                        # External YAML prompts
â”‚   â”‚   â”œâ”€â”€ query_understanding_prompts.yaml
â”‚   â”‚   â”œâ”€â”€ entity_extraction_prompts.yaml  # V8
â”‚   â”‚   â”œâ”€â”€ faq_prompts.yaml                # V8
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ server.py                       # FastAPI server
â”œâ”€â”€ data/                               # Data storage (V8)
â”‚   â”œâ”€â”€ policy_faq.json                 # Bilingual seed data (34 docs)
â”‚   â”œâ”€â”€ knowledge_graph.db              # SQLite graph storage
â”‚   â””â”€â”€ sessions.db                     # Session persistence
â”œâ”€â”€ frontend/                           # Next.js 14
â”œâ”€â”€ config.yaml                         # LLM configurations
â”œâ”€â”€ start_w0_docker.sh                  # Local development
â”œâ”€â”€ start.sh                            # Docker deployment
â””â”€â”€ Architecture.md                     # Detailed architecture
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|------------|
| **Architecture** | **100% Agentic AI** (LLM-First) |
| **Orchestration** | LangGraph (Stateful Multi-Agent) |
| **Backend** | Python 3.11, FastAPI |
| **AI Models** | Cerebras (Qwen 3 32B) |
| **Fallback** | Gemini 2.0 Flash, GPT-4o-mini |
| **RAG** | FAISS (semantic search), SQLite (knowledge graph) |
| **Embeddings** | Qwen3-Embedding-0.6B (1024-dim) |
| **Data Source** | SerpAPI (Amazon), Policy/FAQ JSON |
| **Storage** | SQLite (sessions + knowledge graph) |
| **Frontend** | Next.js 14, TypeScript |

---

## ğŸ“š Documentation

- **[Architecture.md](Architecture.md)** - Complete V7 architecture guide
- **[config.yaml](config.yaml)** - LLM configurations

---

## ğŸš€ Recent Updates

### v8.0.0 - RAG for Policies/FAQs (2025-12-17)
- ğŸ“š **KnowledgeBase**: FAISS-backed semantic search with 34 bilingual documents
- ğŸ•¸ï¸ **KnowledgeGraph**: SQLite graph with 11 entity types, 13 relationship types
- ğŸ¤– **EntityExtractor**: LLM-powered extraction with confidence scoring
- ğŸŒ **Bilingual Support**: EN/VI with automatic language detection
- ğŸ”„ **Hybrid RAG**: Vector search + graph traversal (max_hops=2)
- âš¡ **Auto-Initialization**: Loads policies/FAQs on server startup

### v7.1.0 - 100% Agentic Refinement (2025-12-14)
- ğŸš« **Zero Hardcoded Patterns**: Removed all vietnamese_refinement_patterns
- ğŸ¤– **LLM-Based Detection**: New `is_refinement_only` field determined by LLM
- âœ… **100% Pass Rate**: All evaluation scenarios pass

### v7.0.0 - Session Persistence (2025-12-14)
- ğŸ§  **Session Persistence**: Fixed session_id handling, SQLite storage
- ğŸ“Š **10+ Message Context**: Full conversation retention

### v6.0.0 - Consultative Shopping (2025-12-12)
- ğŸ¤ **Consultative Flow**: Pre-search consultation for vague queries
- ğŸ“Š **Completeness Check**: Routes based on information completeness

### v5.0.0 - 100% Agentic AI (2025-12-12)
- ğŸ¯ **Zero Hardcoded Patterns**: All prompts externalized to YAML
- ğŸ¤– **LLM-Generated Responses**: Dynamic, natural greetings
- ğŸŒ **TranslationService**: LLM-based Vietnamese â†’ English

---

## ğŸ“Š Evaluation Results

| Metric | Result |
|--------|--------|
| **Scenarios** | 10/10 passed (100%) |
| **Routing Accuracy** | 100% |
| **FAQ Accuracy** | Bilingual support verified |
| **RAG Documents** | 34 policies/FAQs loaded |
| **Avg Response Time** | 3.36s |
| **Context Retention** | 10+ messages |

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing`)
5. Open Pull Request

---